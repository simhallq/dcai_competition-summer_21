{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Find duplicates using statistics for each image (https://pillow.readthedocs.io/en/stable/reference/ImageStat.html).\n",
                "# Worked well in the sense that all images it found were real duplicates.\n",
                "# Took almost 2 h to run locally.\n",
                "from tqdm import tqdm\n",
                "import os\n",
                "from PIL import Image, ImageStat\n",
                "from pathlib import Path\n",
                "\n",
                "\n",
                "\n",
                "image_paths = list(Path('../data/').glob('train/**/*.png'))\n",
                "image_paths += list(Path('../data/').glob('val/**/*.png'))\n",
                "\n",
                "duplicate_files = []\n",
                "\n",
                "#this could be improved considerable by storing mean of each image in a dictionary and running a lookup instead.\n",
                "for file_org in tqdm(image_paths):\n",
                "    if not file_org in duplicate_files:\n",
                "        pixels_mean_org = ImageStat.Stat(Image.open(str(file_org))).mean\n",
                "        for file_check in image_paths:\n",
                "            if file_check != file_org:\n",
                "                pix_mean_check = ImageStat.Stat(Image.open(str(file_check))).mean\n",
                "                if pixels_mean_org == pix_mean_check:\n",
                "                    duplicate_files.append((file_org))\n",
                "                    duplicate_files.append((file_check))\n",
                "\n",
                "list(dict.fromkeys(duplicate_files))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "len(duplicate_files)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "98"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 12
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Visuallly inspect the results\n",
                "from IPython.display import display\n",
                "\n",
                "for f in duplicate_files:\n",
                "    display(Image.open(str(f)))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# Save results\n",
                "with open('../duplicate_imgs.txt','w+') as f:\n",
                "    f.writelines('\\n'.join([str(_) for _ in duplicate_files]))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\"\"\"\n",
                "METHOD BELOW DOES NOT WORK. IT MARKED IMAGES AS DUPLICATES THAT WEREN'T DUPLICATES. \n",
                "\n",
                "\n",
                "STILL INTERESTING, SO I KEEP IT :)\n",
                "\n",
                "\"\"\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import cv2\n",
                "\n",
                "def dhash(image, hash_size=8):\n",
                "    #https://www.pyimagesearch.com/2020/04/20/detect-and-remove-duplicate-images-from-a-dataset-for-deep-learning/https://www.pyimagesearch.com/2020/04/20/detect-and-remove-duplicate-images-from-a-dataset-for-deep-learning/\n",
                "\t# convert the image to grayscale and resize the grayscale image,\n",
                "\t# adding a single column (width) so we can compute the horizontal\n",
                "\t# gradient\n",
                "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
                "\tresized = cv2.resize(gray, (hash_size + 1, hash_size))\n",
                "\t# compute the (relative) horizontal gradient between adjacent\n",
                "\t# column pixels\n",
                "\tdiff = resized[:, 1:] > resized[:, :-1]\n",
                "\t# convert the difference image to a hash and return it\n",
                "\treturn sum([2 ** i for (i, v) in enumerate(diff.flatten()) if v])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from pathlib import Path\n",
                "\n",
                "\n",
                "# grab the paths to all images in our input dataset directory and\n",
                "# then initialize our hashes dictionary\n",
                "image_paths = list(Path('../data/').glob('train/**/*.png'))\n",
                "image_paths += list(Path('../data/').glob('val/**/*.png'))\n",
                "\n",
                "print(f\"Deduping {len(image_paths)} images\")\n",
                "#image_paths = Path('../data/val')\n",
                "\n",
                "\n",
                "hashes = {}\n",
                "# loop over our image paths\n",
                "for image_path in image_paths:\n",
                "\t# load the input image and compute the hash\n",
                "\timage = cv2.imread(str(image_path))\n",
                "\th = dhash(image)\n",
                "\t# grab all image paths with that hash, add the current image\n",
                "\t# path to it, and store the list back in the hashes dictionary\n",
                "\tp = hashes.get(h, [])\n",
                "\tp.append(image_path)\n",
                "\thashes[h] = p\n",
                "\n",
                "cnt=0\n",
                "for k,v in hashes.items():\n",
                "\tif len(v)>1:\n",
                "\t\tcnt+=len(v)\n",
                "print(f\"Found {cnt} duplicate images\")\n",
                "\t"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#from matplotlib import pyplot as plt\n",
                "import time\n",
                "\n",
                "from PIL import Image\n",
                "import cv2 \n",
                "from IPython.display import display\n",
                "\n",
                "#Manually check duplicates\n",
                "for k,v in hashes.items():\n",
                "\tif len(v)>1:\n",
                "\t\tprint(len(v))\n",
                "\t\tprint(f\"Dup {v}\")\n",
                "\t\t# for img in v:\n",
                "\t\t# # \tplt.imshow(cv2.imread(str(img)))\n",
                "\t\t# # # plt.imshow(gray)\n",
                "\t\t# # # plt.imshow(gray)\n",
                "\t\t# # #plt.title('my picture')\n",
                "\t\t# # plt.show()\n",
                "\n",
                "\t\t# \timg = cv2.imread(str(img)) # with the OpenCV function imread(), the order of colors is BGR (blue, green, red).\n",
                "\t\t# \t# In Pillow, the order of colors is assumed to be RGB (red, green, blue).\n",
                "\t\t# \t# As we are using Image.fromarray() of PIL module, we need to convert BGR to RGB.\n",
                "\t\t# \timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converting BGR to RGB\n",
                "\t\t# \tdisplay(Image.fromarray(img))\n",
                "\t\t\n",
                "\t\t# time.sleep(60)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import os\n",
                "hashes\n",
                "#remove duplicate images from train-clean\n",
                "data_dir=Path('../data/')\n",
                "cnt_train=0\n",
                "cnt_val=0\n",
                "for k,v in hashes.items():\n",
                "\tif len(v)>1:\n",
                "                for dup in v[1:]:\n",
                "                        for match in (data_dir/'train-clean').glob(f'**/{dup}'):\n",
                "                                os.remove(match)\n",
                "                                cnt_train+=1\n",
                "                        for match in (data_dir/'val-clean').glob(f'**/{dup}'):\n",
                "                                os.remove(match)\n",
                "                                cnt_val+=1\n",
                "print(f\"Removed {cnt_train} images from training set\\nRemoved {cnt_val} images from validation set\")\n",
                "\n",
                "        \n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('tf_m1': conda)"
        },
        "interpreter": {
            "hash": "209cf731f3cd8258de834293906c865ec04610b0152d9d85d8232254c3973610"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}